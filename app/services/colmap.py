import subprocess
import time
import logging
import json
import os
from pathlib import Path

logger = logging.getLogger(__name__)

# Check if running in Docker mode
USE_DOCKER = os.getenv("USE_DOCKER_WORKER", "true").lower() == "true"


def run_colmap_docker(project_id: str, params: dict = None) -> None:
    """
    Run COLMAP via Docker worker.
    """
    from app.config import DATA_DIR
    
    params_json = json.dumps(params or {})
    # DATA_DIR is now /path/to/websplat-backend/data/projects
    # Mount parent: /path/to/websplat-backend/data -> /data
    data_dir = DATA_DIR.parent

    # Ensure a writable cache directory exists on the host and will be mounted
    cache_dir = data_dir / ".cache"
    try:
        cache_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.warning(f"Could not create cache dir {cache_dir}: {e}")
    
    # Run container as the host backend user to avoid root-owned output files.
    uid = os.getuid()
    gid = os.getgid()
    user_flag = ["-u", f"{uid}:{gid}"]

    cmd = [
        "docker", "run", "--rm",
        *user_flag,
        "--gpus", "all",  # Enable GPU access
        # Mount project data and a writable cache into the container so PyTorch
        # can build extensions without trying to write to '/.cache' inside root.
        "-v", f"{data_dir}:/data",
        "-v", f"{cache_dir}:/data/.cache",
        "-e", "TORCH_EXTENSIONS_DIR=/data/.cache/torch_extensions",
        "-e", "XDG_CACHE_HOME=/data/.cache",
        "bimba3d-worker:latest",
        project_id,
        "--data-dir", "/data/projects",
        "--params", params_json
    ]
    
    logger.info(f"Running Docker worker: {' '.join(cmd)}")

    # Stream container output into the project's processing.log so the frontend
    # can display live logs. Avoid capture_output which buffers large output
    # and can cause deadlocks for long-running containers.
    from app.services import storage as _storage
    try:
        project_dir = _storage.get_project_dir(project_id)
        logs_file = project_dir / "processing.log"
        logs_file.parent.mkdir(parents=True, exist_ok=True)
    except Exception:
        logs_file = None

    try:
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
        assert proc.stdout is not None
        for line in proc.stdout:
            line = line.rstrip('\n')
            try:
                logger.info(line)
            except Exception:
                pass
            if logs_file is not None:
                try:
                    with open(logs_file, 'a') as f:
                        f.write(line + '\n')
                except Exception:
                    pass
        rc = proc.wait()
        if rc != 0:
            # If container was killed by OOM (137) provide a helpful status message
            try:
                from app.services import status as _status
                if project_id and rc == 137:
                    msg = (
                        "Worker container exited with code 137 (out of memory). "
                        "This usually means the host ran out of RAM or the GPU ran out of memory. "
                        "Try reducing the number of initialized Gaussians (Trainer -> Init Gaussians), "
                        "or lower COLMAP `max_image_size` and `mapper_num_threads` in the Process configuration."
                    )
                    try:
                        _status.update_status(project_id, "failed", error=msg, message=msg)
                    except Exception:
                        pass
            except Exception:
                pass
            raise subprocess.CalledProcessError(rc, cmd)
    except subprocess.CalledProcessError as e:
        logger.error(f"Docker worker failed: returncode={getattr(e, 'returncode', None)}")
        raise


def _run_cmd_with_retry(cmd: list[str], retries: int = 3, delay_sec: float = 2.0):
    last_err = None
    for attempt in range(1, retries + 1):
        try:
            res = subprocess.run(cmd, check=True, capture_output=True, text=True)
            if res.stdout:
                logger.info(res.stdout.strip())
            if res.stderr:
                logger.debug(res.stderr.strip())
            return
        except subprocess.CalledProcessError as e:
            stderr = (e.stderr or "").lower()
            last_err = e
            if "database is locked" in stderr or "busy" in stderr:
                logger.warning(f"SQLite busy/locked (attempt {attempt}/{retries}). Retrying after {delay_sec}s...")
                time.sleep(delay_sec)
                continue
            logger.error(f"Command failed: {cmd}\nSTDERR: {e.stderr}")
            raise
    logger.error(f"Command failed after retries: {cmd}\nERR: {last_err}")
    raise last_err


def _cleanup_sqlite_sidecars(db_path: Path):
    for suffix in ("-wal", "-shm"):
        sidecar = db_path.with_name(db_path.name + suffix)
        if sidecar.exists():
            try:
                sidecar.unlink()
                logger.info(f"Removed stale SQLite sidecar: {sidecar}")
            except Exception as e:
                logger.warning(f"Failed to remove sidecar {sidecar}: {e}")


def run_colmap(image_dir: Path, output_dir: Path, params: dict | None = None):
    """
    Run COLMAP feature extraction, matching, and sparse reconstruction.
    
    Args:
        image_dir: Directory containing input images
        output_dir: Directory for COLMAP outputs
        
    Returns:
        Path to sparse reconstruction directory
        
    Raises:
        subprocess.CalledProcessError: If COLMAP commands fail
        FileNotFoundError: If COLMAP is not installed
    """
    db_path = output_dir / "database.db"
    sparse_dir = output_dir / "sparse"
    
    sparse_dir.mkdir(parents=True, exist_ok=True)
    
    # Remove existing database and sidecars to prevent locking issues
    if db_path.exists():
        logger.info(f"Removing existing database: {db_path}")
        try:
            db_path.unlink()
        except Exception:
            db_path.write_bytes(b"")
    _cleanup_sqlite_sidecars(db_path)
    
    try:
        # Allow optional tuning via params.colmap
        p = params.get("colmap", {}) if isinstance(params, dict) else {}

        # 1️⃣ Feature extraction
        logger.info("Running COLMAP feature extraction...")
        feat_cmd = [
            "colmap", "feature_extractor",
            "--database_path", str(db_path),
            "--image_path", str(image_dir),
            "--ImageReader.camera_model", "OPENCV",
            "--ImageReader.single_camera", "1",
        ]
        if p.get("max_image_size"):
            feat_cmd += ["--SiftExtraction.max_image_size", str(p.get("max_image_size"))]
        else:
            feat_cmd += ["--SiftExtraction.max_image_size", "1600"]
        if p.get("peak_threshold") is not None:
            feat_cmd += ["--SiftExtraction.peak_threshold", str(p.get("peak_threshold"))]
        else:
            feat_cmd += ["--SiftExtraction.peak_threshold", "0.01"]

        _run_cmd_with_retry(feat_cmd)
        logger.info("✓ Feature extraction completed")

        # 2️⃣ Feature matching
        logger.info("Running COLMAP feature matching...")
        guided = p.get("guided_matching")
        matching_type = p.get("matching_type", "exhaustive")
        if matching_type == "sequential":
            match_cmd = [
                "colmap", "sequential_matcher",
                "--database_path", str(db_path),
            ]
        else:
            match_cmd = [
                "colmap", "exhaustive_matcher",
                "--database_path", str(db_path),
            ]
        if guided is not None:
            match_cmd += ["--SiftMatching.guided_matching", "1" if guided else "0"]
        else:
            match_cmd += ["--SiftMatching.guided_matching", "1"]

        _run_cmd_with_retry(match_cmd)
        logger.info("✓ Feature matching completed")

        # 3️⃣ Sparse reconstruction
        logger.info("Running COLMAP sparse reconstruction (mapper)...")
        mapper_cmd = [
            "colmap", "mapper",
            "--database_path", str(db_path),
            "--image_path", str(image_dir),
            "--output_path", str(sparse_dir),
            "--Mapper.ba_refine_principal_point", "1",
            "--Mapper.ba_refine_focal_length", "1",
            "--Mapper.ba_refine_extra_params", "1",
        ]
        if p.get("mapper_num_threads"):
            mapper_cmd += ["--Mapper.num_threads", str(p.get("mapper_num_threads"))]
        else:
            # Default to 2 mapper threads to reduce memory usage on typical hosts
            mapper_cmd += ["--Mapper.num_threads", "2"]

        _run_cmd_with_retry(mapper_cmd)
        logger.info("✓ Sparse reconstruction completed")
        
        # Verify outputs
        if not (sparse_dir / "0").exists():
            raise FileNotFoundError(f"COLMAP reconstruction failed - no output in {sparse_dir}")
        
        logger.info(f"COLMAP outputs saved to: {sparse_dir}")
        # Attempt to generate compact points.bin for easier web consumption
        try:
            from app.services import pointsbin
            # Iterate recon dirs and convert any with COLMAP points
            for d in sorted([p for p in sparse_dir.iterdir() if p.is_dir()]):
                try:
                    cnt = pointsbin.convert_colmap_recon_to_pointsbin(d)
                    if cnt:
                        logger.info(f"Converted COLMAP recon {d} -> points.bin ({cnt} points)")
                except Exception as e:
                    logger.warning(f"Failed to convert recon {d} to points.bin: {e}")
        except Exception:
            # Non-fatal if converter missing or fails; continue returning sparse_dir
            logger.debug("pointsbin converter not available or failed")

        return sparse_dir
        
    except FileNotFoundError as e:
        logger.error(f"COLMAP not found. Install with: apt-get install colmap")
        raise
    except subprocess.CalledProcessError as e:
        logger.error(f"COLMAP command failed: {e.stderr}")
        raise
